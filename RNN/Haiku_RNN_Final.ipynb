{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9de8SHl43gg",
        "outputId": "17dc14e8-13d7-400a-fc77-79b629aa855d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import cmudict\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HaikuDataset(Dataset):\n",
        "    def __init__(self, haikus: List[List[str]], word2idx: dict, max_len: int):\n",
        "        self.haikus = haikus\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.haikus)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        haiku = self.haikus[idx]\n",
        "        indices = [self.word2idx[word] for word in haiku]\n",
        "        padded = indices + [self.word2idx['<PAD>']] * (self.max_len - len(indices))\n",
        "        return torch.LongTensor(padded[:-1]), torch.LongTensor(padded[1:])\n",
        "\n",
        "class HaikuRNN(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int):\n",
        "        super(HaikuRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "cxfNH7kZ47xD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(file_path: str) -> Tuple[List[List[str]], dict, dict]:\n",
        "    df = pd.read_csv(file_path)\n",
        "    haikus = []\n",
        "    tokenizer = RegexpTokenizer(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        haiku = f\"{row['0']} {row['1']} {row['2']}\"\n",
        "        tokens = tokenizer.tokenize(haiku.lower())\n",
        "        haikus.append(tokens)\n",
        "\n",
        "    vocab = set()\n",
        "    for haiku in haikus:\n",
        "        vocab.update(haiku)\n",
        "    vocab.add('<PAD>')\n",
        "    vocab.add('<SOS>')\n",
        "    vocab.add('<EOS>')\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "    return haikus, word2idx, idx2word\n",
        "\n",
        "\n",
        "def count_syllables(word: str, d: dict) -> int:\n",
        "    try:\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
        "    except KeyError:\n",
        "        return 1\n",
        "\n",
        "def generate_haiku_from_model(model: HaikuRNN, word2idx: dict, idx2word: dict,\n",
        "                              syllable_dict: dict, device: str) -> str:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        current_word = '<SOS>'\n",
        "        hidden = None\n",
        "\n",
        "        line1 = []\n",
        "        syllables = 0\n",
        "        attempts = 0\n",
        "        while syllables < 5 and attempts < 10:\n",
        "            input_tensor = torch.LongTensor([word2idx[current_word]]).unsqueeze(0).to(device)\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            probs = torch.softmax(output[0, -1], dim=0)\n",
        "            next_word_idx = torch.multinomial(probs, 1).item()\n",
        "            next_word = idx2word[next_word_idx]\n",
        "            if next_word not in ['<PAD>', '<SOS>', '<EOS>']:\n",
        "                syllables += count_syllables(next_word, syllable_dict)\n",
        "                line1.append(next_word)\n",
        "            current_word = next_word\n",
        "            attempts += 1\n",
        "\n",
        "        line2 = []\n",
        "        syllables = 0\n",
        "        while syllables < 7 and attempts <= 10:\n",
        "            input_tensor = torch.LongTensor([word2idx[current_word]]).unsqueeze(0).to(device)\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            probs = torch.softmax(output[0, -1], dim=0)\n",
        "            next_word_idx = torch.multinomial(probs, 1).item()\n",
        "            next_word = idx2word[next_word_idx]\n",
        "            if next_word not in ['<PAD>', '<SOS>', '<EOS>']:\n",
        "                syllables += count_syllables(next_word, syllable_dict)\n",
        "                line2.append(next_word)\n",
        "            current_word = next_word\n",
        "            attempts += 1\n",
        "\n",
        "        line3 = []\n",
        "        syllables = 0\n",
        "        while syllables < 5 and attempts <= 10:\n",
        "            input_tensor = torch.LongTensor([word2idx[current_word]]).unsqueeze(0).to(device)\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            probs = torch.softmax(output[0, -1], dim=0)\n",
        "            next_word_idx = torch.multinomial(probs, 1).item()\n",
        "            next_word = idx2word[next_word_idx]\n",
        "            if next_word not in ['<PAD>', '<SOS>', '<EOS>']:\n",
        "                syllables += count_syllables(next_word, syllable_dict)\n",
        "                line3.append(next_word)\n",
        "            current_word = next_word\n",
        "            attempts += 1\n",
        "\n",
        "        return ' '.join(line1) + '\\n' + ' '.join(line2) + '\\n' + ' '.join(line3)"
      ],
      "metadata": {
        "id": "KWGS-F_N4-Zy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- HaikuGenerator Class -----\n",
        "class HaikuGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_file: str,\n",
        "        batch_size: int = 32,\n",
        "        embedding_dim: int = 100,\n",
        "        hidden_dim: int = 256,\n",
        "        num_layers: int = 2\n",
        "    ):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.haikus, self.word2idx, self.idx2word = load_and_preprocess_data(csv_file)\n",
        "\n",
        "        self.syllable_dict = cmudict.dict()\n",
        "\n",
        "        self.max_len = max(len(haiku) for haiku in self.haikus) + 2  # +2 for special tokens\n",
        "        self.dataset = HaikuDataset(self.haikus, self.word2idx, self.max_len)\n",
        "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        vocab_size = len(self.word2idx)\n",
        "        self.model = HaikuRNN(vocab_size, embedding_dim, hidden_dim, num_layers).to(self.device)\n",
        "\n",
        "\n",
        "    def train(self, num_epochs: int = 10, lr: float = 0.001, save_path: str = None):\n",
        "        self.model.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0\n",
        "            for inputs, targets in tqdm(self.dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                output, _ = self.model(inputs)\n",
        "                loss = criterion(output.view(-1, len(self.word2idx)), targets.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(self.dataloader)\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
        "            self.model.train()\n",
        "\n",
        "    def generate_haiku(self) -> str:\n",
        "        self.model.eval()\n",
        "        haiku = generate_haiku_from_model(\n",
        "            self.model,\n",
        "            self.word2idx,\n",
        "            self.idx2word,\n",
        "            self.syllable_dict,\n",
        "            self.device\n",
        "        )\n",
        "        return haiku"
      ],
      "metadata": {
        "id": "p5tgLUp55D2P"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/RNN_Haiku_state.pth'"
      ],
      "metadata": {
        "id": "ssYtiU3ZGgJJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/all_haiku.csv'\n",
        "save_path = '/content/drive/MyDrive/RNN_Haiku_state.pth'\n",
        "\n",
        "generator = HaikuGenerator(\n",
        "    csv_file=csv_path,\n",
        "    batch_size=4,         # 4\n",
        "    embedding_dim=32,       # 32\n",
        "    hidden_dim=16,         # 16\n",
        "    num_layers=1            # 1\n",
        ")\n",
        "generator.train(num_epochs=3, lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nSkfkKY5I-H",
        "outputId": "6a1ce3c3-767b-4585-f6e1-79f0e9fdcf41"
      },
      "execution_count": 28,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 36031/36031 [16:15<00:00, 36.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3, Loss: 0.2396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 36031/36031 [16:14<00:00, 36.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/3, Loss: 0.1842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 36031/36031 [16:14<00:00, 36.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/3, Loss: 0.1814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator)\n",
        "torch.save(generator.model.state_dict(), save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3Bz3aS01UEA",
        "outputId": "0f838884-172a-494c-9d32-5a1c627d80a8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.HaikuGenerator object at 0x7b83387c5c50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(generator.word2idx)\n",
        "embedding_dim = 32\n",
        "hidden_dim = 16\n",
        "num_layers = 1\n",
        "\n",
        "generator = HaikuGenerator(\n",
        "    csv_file=csv_path,\n",
        "    batch_size=4,\n",
        "    embedding_dim=32,\n",
        "    hidden_dim=16,\n",
        "    num_layers=1\n",
        ")\n",
        "\n",
        "generator.model = HaikuRNN(vocab_size, embedding_dim, hidden_dim, num_layers).to(generator.device)\n",
        "generator.model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "generator.model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgiXwjqT_JQQ",
        "outputId": "97beae84-badf-40f3-e10d-2d0c84ef54e3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Haiku:\n",
            " are a fearful of\n",
            "that yikes get free and something\n",
            "gaga for too shit i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "haiku = generator.generate_haiku()\n",
        "print(\"Generated Haiku:\\n\", haiku)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgjyTp4PB5rJ",
        "outputId": "fbcbbe65-b831-4450-e3a0-7ff9f0d9b797"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Haiku:\n",
            " equal on the mountains\n",
            "hotel growing which the pea\n",
            "blue found the royals\n"
          ]
        }
      ]
    }
  ]
}